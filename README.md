# i310D_Assignment10
Coding Bias

According to PBS News (https://www.pbs.org/newshour/politics/women-reflect-on-sexist-slur-that-often-goes-unpunished) the word "Bitch", which is considered a sexist slur is a word that often goes "unpunished". The word bitch also means female dog, but more often than not it is used in a derogatory context. If the word "bitch" is utilized in a comment, then it will be considered non-toxic more than toxic, because it is a word that has been decensitized and glossed over by society.

From looking at the data, bitch in some contexts is not considered toxic, but when it is paired with other sexist slurs or other profanity such as "fuck, it is considered toxic.

For professional purposes, the word "bitch" will be reffered to as "b"

### Results
After analying this data set, it shows that for every non-toxic comment there is also a comment that is considered non toxic with the word bitch in it. There is almost a split difference in the type of labeling. It does contradict my hypothesis because there is a 12% difference of a comment being labeled toxic more often than not. A bigger sample size could always skew the data more.

Some biases I think include what other words the comment is paired with. From manualy parsing the data, I saw that when bitch was accompanied by other sexist slurs or homophobic language it was flagged more. Another variable I saw was when a comment was in all caps it was labeled toxic even if it was just a random string of words.

I think my results are the way they are because of the data set that I analized. It came pre-labeled so I do not know the paramaters of what "toxic" was for the labeler.