{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85291d4f",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "Hypothesis: According to PBS News (https://www.pbs.org/newshour/politics/women-reflect-on-sexist-slur-that-often-goes-unpunished) the word \"Bitch\", which is considered a sexist slur is a word that often goes \"unpunished\". The word bitch also means female dog, but more often than not it is used in a derogatory context. If the word \"bitch\" is utilized in a comment, then it will be considered non-toxic more than toxic, because it is a word that has been decensitized and glossed over by society. \n",
    "\n",
    "From looking at the data, bitch in some contexts is not considered toxic, but when it is paired with other sexist slurs or other profanity such as \"fuck, it is considered toxic.\n",
    "\n",
    "For professional purposes, the word \"bitch\" will be reffered to as \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb521ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8464917",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e73b4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Number of Comments labeled as 'Toxic' with the word 'Bitch' in it: 4\n",
      "Number of Comments labeled as 'non-Toxic' with the word 'Bitch' in it: 1\n"
     ]
    }
   ],
   "source": [
    "#Parsing through sample data set and choosing data points to establish threshold of average toxicity score\n",
    "\n",
    "sample_comments = pd.read_csv(\"Sample_labaled_data.csv\")\n",
    "# sample_comments.head()\n",
    "toxicity = list(sample_comments[\"toxic\"])\n",
    "comments = list(sample_comments[\"comment_text\"])\n",
    "\n",
    "toxic_comments = []\n",
    "neutral_comments = []\n",
    "\n",
    "for i in range(len(comments)):\n",
    "    toxic = toxicity[i]\n",
    "    comment = comments[i]\n",
    "    \n",
    "    if toxic == \"yes\":\n",
    "        toxic_comments.append(comment)\n",
    "    else:\n",
    "        neutral_comments.append(comment)\n",
    "        \n",
    "toxic_comments = toxic_comments[300:360]\n",
    "neutral_comments = neutral_comments[300:360]\n",
    "\n",
    "        \n",
    "# print(neutral_comments)\n",
    "print(\"done\")\n",
    "\n",
    "b_toxic_counter = 0\n",
    "for comment in toxic_comments:\n",
    "    if \"bitch\" in comment: \n",
    "        b_toxic_counter += 1\n",
    "        \n",
    "print(f\"Number of Comments labeled as 'Toxic' with the word 'Bitch' in it: {b_toxic_counter}\")\n",
    "\n",
    "b_neutral_counter = 0\n",
    "for comment in neutral_comments:\n",
    "    if \"bitch\" in comment: \n",
    "        b_neutral_counter += 1\n",
    "        \n",
    "print(f\"Number of Comments labeled as 'non-Toxic' with the word 'Bitch' in it: {b_neutral_counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10e96a",
   "metadata": {},
   "source": [
    "## Threshold and Fidnings for test data\n",
    "In this 60 comment data set, there is a 4:1 ratio of the word bitch being considered toxic versus not. This sample directly contradics my hypothesis but it is also taken from 30 data points. My bigger sample data which extracted all of the comments with the word \"Bitch\" and their label in it is around ~200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa5104bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyBwsdsPNe0OU5LKWvCgjNxvrQ3NpsVJYdQ'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a5be115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments labeled as 'Toxic' with the word 'Bitch' in it: 117\n",
      "Number of Comments labeled as 'non-toxic' with the word 'Bitch' in it: 90\n"
     ]
    }
   ],
   "source": [
    "# sample_comments = pd.read_csv(\"Sample_labaled_data.csv\")\n",
    "sample_comments = pd.read_csv(\"B_only_dataset.csv\")\n",
    "\n",
    "# sample_comments.head()\n",
    "toxicity = list(sample_comments[\"toxic\"])\n",
    "comments = list(sample_comments[\"comment_text\"])\n",
    "\n",
    "toxic_comments = []\n",
    "neutral_comments = []\n",
    "\n",
    "for i in range(len(comments)):\n",
    "    toxic = toxicity[i]\n",
    "    comment = comments[i]\n",
    "    \n",
    "    if toxic == \"yes\":\n",
    "        toxic_comments.append(comment)\n",
    "    else:\n",
    "        neutral_comments.append(comment)\n",
    "\n",
    "       \n",
    "# print(neutral_comments)\n",
    "\n",
    "b_toxic_counter = 0\n",
    "for comment in toxic_comments:\n",
    "    if \"bitch\" in comment: \n",
    "        b_toxic_counter += 1\n",
    "        \n",
    "print(f\"Number of Comments labeled as 'Toxic' with the word 'Bitch' in it: {b_toxic_counter}\")\n",
    "\n",
    "b_neutral_counter = 0\n",
    "for comment in neutral_comments:\n",
    "    if \"bitch\" in comment: \n",
    "        b_neutral_counter += 1\n",
    "        \n",
    "print(f\"Number of Comments labeled as 'non-toxic' with the word 'Bitch' in it: {b_neutral_counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23886f01",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "After analying this data set, it shows that for every non-toxic comment there is also a comment that is considered non toxic with the word bitch in it. There is almost a split difference in the type of labeling. It does contradict my hypothesis because there is a 12% difference of a comment being labeled toxic more often than not. A bigger sample size could always skew the data more.\n",
    "\n",
    "Some biases I think include what other words the comment is paired with. From manualy parsing the data, I saw that when bitch was accompanied by other sexist slurs or homophobic language it was flagged more. Another variable I saw was when a comment was in all caps it was labeled toxic even if it was just a random string of words.\n",
    "\n",
    "I think my results are the way they are because of the data set that I analized. It came pre-labeled so I do not know the paramaters of what \"toxic\" was for the labeler. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
